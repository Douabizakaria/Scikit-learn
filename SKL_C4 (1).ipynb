{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bbd62a-3226-424c-9cf3-aa2297c7f548",
      "metadata": {
        "id": "98bbd62a-3226-424c-9cf3-aa2297c7f548"
      },
      "outputs": [],
      "source": [
        "# Based on the datacamp course : https://www.datacamp.com/tutorial/decision-tree-classification-python\n",
        "# Modified by Mehdi Ammi, Univ. Paris 8"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6a30f6-4952-43c8-8dde-7a16d7493eea",
      "metadata": {
        "id": "2f6a30f6-4952-43c8-8dde-7a16d7493eea"
      },
      "source": [
        "# Scikit-Learn: Decision Tree (DT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee1cf1e-f8a1-472a-bfd4-9c19e7ad5b7e",
      "metadata": {
        "id": "dee1cf1e-f8a1-472a-bfd4-9c19e7ad5b7e"
      },
      "source": [
        "## Introduction to Decision Tree\n",
        "\n",
        "A decision tree is a flowchart-like tree structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\n",
        "\n",
        "The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision-making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n",
        "\n",
        "![DT_3.png](attachment:9321d0e9-b3f3-4a23-b258-de52def6dfe5.png)\n",
        "\n",
        "A decision tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as with a neural network. Its training time is faster compared to the neural network algorithm.\n",
        "\n",
        "The time complexity of decision trees is a function of the number of records and attributes in the given data. The decision tree is a distribution-free or non-parametric method which does not depend upon probability distribution assumptions. Decision trees can handle high-dimensional data with good accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7363b8ea-cfd7-4e47-8158-51e95799e767",
      "metadata": {
        "id": "7363b8ea-cfd7-4e47-8158-51e95799e767"
      },
      "source": [
        "## How Does the Decision Tree Algorithm Work?\n",
        "\n",
        "The basic idea behind any decision tree algorithm is as follows:\n",
        "\n",
        "1. Select the best attribute using Attribute Selection Measures (ASM) to split the records.\n",
        "2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
        "3. Start tree building by repeating this process recursively for each child until one of the conditions will match:\n",
        " - All the tuples belong to the same attribute value.\n",
        " - There are no more remaining attributes.\n",
        " - There are no more instances.\n",
        "\n",
        " ![DT_4.png](attachment:2ccfaba9-a28f-42b0-88d6-f07c6a814823.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b50ab8a0-2b48-4992-9b6f-5570f173d7c3",
      "metadata": {
        "id": "b50ab8a0-2b48-4992-9b6f-5570f173d7c3"
      },
      "source": [
        "## Attribute Selection Measures\n",
        "Attribute selection measure is a heuristic for selecting the splitting criterion that partitions data in the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature (or attribute) by explaining the given dataset. The best score attribute will be selected as a splitting attribute. In the case of a continuous-valued attribute, split points for branches also need to define. The most popular selection measures are Information Gain, Gain Ratio, and Gini Index.\n",
        "\n",
        "### Information Gain\n",
        "\n",
        "Claude Shannon invented the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy is referred to as the randomness or the impurity in a system. In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. Information gain computes the difference between entropy before the split and average entropy after the split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n",
        "\n",
        "![DT_5.png](attachment:0c621358-84b2-466d-8000-a3ac72889b1f.png)\n",
        "\n",
        "Where Pi is the probability that an arbitrary tuple in D belongs to class Ci.\n",
        "\n",
        "![DT_6.png](attachment:c50f8fab-2ca7-4df2-8762-1ae73e253423.png)\n",
        "\n",
        "![DT_7.png](attachment:b2eeea07-32f3-4a79-8da1-080f82285d79.png)\n",
        "\n",
        "Where:\n",
        " - Info(D) is the average amount of information needed to identify the class label of a tuple in D.\n",
        " - |Dj|/|D| acts as the weight of the jth partition.\n",
        " - InfoA(D) is the expected information required to classify a tuple from D based on the partitioning by A.\n",
        "\n",
        " The attribute A with the highest information gain, Gain(A), is chosen as the splitting attribute at node N()."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55fc84dc-aaf4-45b0-8476-1163e49e3979",
      "metadata": {
        "id": "55fc84dc-aaf4-45b0-8476-1163e49e3979"
      },
      "source": [
        "### Gain Ratio\n",
        "\n",
        "Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier, such as customer_ID, that has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n",
        "\n",
        "C4.5, an improvement of ID3, uses an extension to information gain known as the gain ratio. Gain ratio handles the issue of bias by normalizing the information gain using Split Info. Java implementation of the C4.5 algorithm is known as J48, which is available in WEKA data mining tool.\n",
        "\n",
        "![DT_8.png](attachment:95a5af7e-9321-42aa-a5ad-dc1b9d4c6171.png)\n",
        "\n",
        "Where:\n",
        " - |Dj|/|D| acts as the weight of the jth partition.\n",
        " - v is the number of discrete values in attribute A.\n",
        "\n",
        "The gain ratio can be defined as\n",
        "\n",
        "![DT_9.png](attachment:ce38e097-c84d-4758-9bc4-8a2140f3d4e5.png)\n",
        "\n",
        "The attribute with the highest gain ratio is chosen as the splitting attribute (Source).\n",
        "\n",
        "### Gini index\n",
        "\n",
        "Another decision tree algorithm CART (Classification and Regression Tree) uses the Gini method to create split points.\n",
        "\n",
        "![DT_10.png](attachment:67f6c6db-f794-4049-a593-6d7466876587.png)\n",
        "\n",
        "Where pi is the probability that a tuple in D belongs to class Ci.\n",
        "\n",
        "The Gini Index considers a binary split for each attribute. You can compute a weighted sum of the impurity of each partition. If a binary split on attribute A partitions data D into D1 and D2, the Gini index of D is:\n",
        "\n",
        "![DT_11.png](attachment:b59d478b-9d84-4cd0-a37c-4f165852ff02.png)\n",
        "\n",
        "In the case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split point, and a point with a smaller gini index is chosen as the splitting point.\n",
        "\n",
        "![DT_12.png](attachment:92948b7b-0c86-4cdd-9b3a-fb661b909bed.png)\n",
        "\n",
        "The attribute with the minimum Gini index is chosen as the splitting attribute.\n",
        "\n",
        "#### Advantages of Decision Trees\n",
        "- **Simplicity and Interpretability**: Decision trees are easy to understand and visualize. Even individuals without a background in data science can interpret the decision rules generated by a tree.\n",
        "- **Flexibility**: Decision trees can handle both categorical and numerical data and do not require scaling of data.\n",
        "- **Handling Missing Data**: Decision trees can manage missing values implicitly by imputing them during the splitting process.\n",
        "\n",
        "#### Disadvantages of Decision Trees\n",
        "- **Overfitting**: Decision trees can easily overfit the training data, especially if the tree becomes very complex.\n",
        "- **Data Sensitivity**: Small variations in the data can result in completely different trees. Techniques like bagging and random forests can mitigate this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b6347b-e6a8-4a1e-95e1-20d2e99dab7c",
      "metadata": {
        "id": "f2b6347b-e6a8-4a1e-95e1-20d2e99dab7c"
      },
      "source": [
        "## Importing Required Libraries\n",
        "\n",
        "Let's first load the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6c1911-e7dd-41f8-b680-798c506ebfdf",
      "metadata": {
        "id": "ba6c1911-e7dd-41f8-b680-798c506ebfdf"
      },
      "outputs": [],
      "source": [
        "# Installs the 'six' library, which provides utilities for writing code that is compatible with both Python 2 and Python 3.\n",
        "!pip install six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb13fa7-98ed-4bd6-be34-eda0e366ea41",
      "metadata": {
        "id": "5fb13fa7-98ed-4bd6-be34-eda0e366ea41"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "\n",
        "# Import files module from google.colab for file uploading\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db4fa422-5c8f-470d-9ede-5d0d8f31fba1",
      "metadata": {
        "id": "db4fa422-5c8f-470d-9ede-5d0d8f31fba1"
      },
      "source": [
        "## Loading Data\n",
        "Let's first load the required Pima Indian Diabetes dataset using pandas' read CSV function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f618ae9c-f110-45b9-8284-7f434fa87246",
      "metadata": {
        "id": "f618ae9c-f110-45b9-8284-7f434fa87246"
      },
      "outputs": [],
      "source": [
        "# Prompt user to upload files and store the uploaded files in a variable\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Define column names for the dataset\n",
        "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
        "\n",
        "# load dataset\n",
        "pima = pd.read_csv(\"diabetes.csv\", header=None, names=col_names)\n",
        "\n",
        "# Remove the first line !\n",
        "#pima = pima.drop(0)\n",
        "\n",
        "pima.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f7bb24-441d-4699-bea7-ab5bb7fe936c",
      "metadata": {
        "id": "38f7bb24-441d-4699-bea7-ab5bb7fe936c"
      },
      "source": [
        "|index|pregnant|glucose|bp|skin|insulin|bmi|pedigree|age|label|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|1|6|148|72|35|0|33\\.6|0\\.627|50|1|\n",
        "|2|1|85|66|29|0|26\\.6|0\\.351|31|0|\n",
        "|3|8|183|64|0|0|23\\.3|0\\.672|32|1|\n",
        "|4|1|89|66|23|94|28\\.1|0\\.167|21|0|\n",
        "|5|0|137|40|35|168|43\\.1|2\\.288|33|1|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8189a954-5504-4132-a8ef-6096ebd8e979",
      "metadata": {
        "id": "8189a954-5504-4132-a8ef-6096ebd8e979"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "Here, you need to divide given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d120af28-c0cc-464b-8ac3-9291170b89c4",
      "metadata": {
        "id": "d120af28-c0cc-464b-8ac3-9291170b89c4"
      },
      "outputs": [],
      "source": [
        "#split dataset in features and target variable\n",
        "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.label # Target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6cb184-1904-4fa9-990e-86cc400fd9ec",
      "metadata": {
        "id": "2b6cb184-1904-4fa9-990e-86cc400fd9ec"
      },
      "source": [
        "Splitting Data\n",
        "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
        "\n",
        "Let's split the dataset by using the function train_test_split(). You need to pass three parameters features; target, and test_set size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb88b6c-4e66-46cc-9f08-a32c6bd11172",
      "metadata": {
        "id": "0bb88b6c-4e66-46cc-9f08-a32c6bd11172"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb02619-d426-4008-ba6e-cacbaf6636a0",
      "metadata": {
        "id": "bfb02619-d426-4008-ba6e-cacbaf6636a0"
      },
      "source": [
        "## Building Decision Tree Model\n",
        "\n",
        "Let's create a decision tree model using Scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da770ff0-f935-4725-aa63-ba3c1cd4d1b6",
      "metadata": {
        "id": "da770ff0-f935-4725-aa63-ba3c1cd4d1b6"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70884e0-4cd8-4b31-8e68-f5bc750494b3",
      "metadata": {
        "id": "e70884e0-4cd8-4b31-8e68-f5bc750494b3"
      },
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "Let's estimate how accurately the classifier or model can predict the type of cultivars.\n",
        "\n",
        "Accuracy can be computed by comparing actual test set values and predicted values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0bfa0e-85f8-4f44-b21e-b77cc8582652",
      "metadata": {
        "id": "3f0bfa0e-85f8-4f44-b21e-b77cc8582652"
      },
      "outputs": [],
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838a9418-25d4-409e-87da-ff6a13150222",
      "metadata": {
        "id": "838a9418-25d4-409e-87da-ff6a13150222"
      },
      "outputs": [],
      "source": [
        ">>\n",
        "Accuracy: 0.6926406926406926"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ad963c-a3c9-464a-aa96-b599b3bd42e8",
      "metadata": {
        "id": "39ad963c-a3c9-464a-aa96-b599b3bd42e8"
      },
      "source": [
        "We got a classification rate of 67.53%, which is considered as good accuracy. You can improve this accuracy by tuning the parameters in the decision tree algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e91cbb43-dd21-4a1b-bbcd-2b912d4e51b1",
      "metadata": {
        "id": "e91cbb43-dd21-4a1b-bbcd-2b912d4e51b1"
      },
      "source": [
        "## Visualizing Decision Trees\n",
        "\n",
        "You can use Scikit-learn's export_graphviz function for display the tree within a Jupyter notebook. For plotting the tree, you also need to install graphviz and pydotplus.\n",
        "\n",
        "pip install graphviz\n",
        "\n",
        "pip install pydotplus\n",
        "\n",
        "The export_graphviz function converts the decision tree classifier into a dot file, and pydotplus converts this dot file to png or displayable form on Jupy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becbf379-42f0-4517-baaa-5b7ebc3b1802",
      "metadata": {
        "id": "becbf379-42f0-4517-baaa-5b7ebc3b1802"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "from sklearn.tree import export_graphviz  # Function to export a decision tree in DOT format\n",
        "from six import StringIO  # To handle string input/output\n",
        "from IPython.display import Image  # To display images in IPython notebooks\n",
        "import pydotplus  # To handle DOT files and create graph images\n",
        "\n",
        "# Create a StringIO object to hold the DOT data\n",
        "dot_data = StringIO()\n",
        "\n",
        "# Export the decision tree (clf) to DOT format and write it to the StringIO object\n",
        "export_graphviz(clf, out_file=dot_data,\n",
        "                filled=True, rounded=True,  # Fill nodes with colors and round their corners\n",
        "                special_characters=True,  # Use special characters in the labels\n",
        "                feature_names=feature_cols,  # Use feature column names in the labels\n",
        "                class_names=['0', '1'])  # Class names for the target variable\n",
        "\n",
        "# Use pydotplus to create a graph from the DOT data\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "\n",
        "# Write the graph to a PNG file\n",
        "graph.write_png('diabetes.png')\n",
        "\n",
        "# Display the graph as a PNG image in the notebook\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fecaf883-f724-4ba6-9a4b-7924da5e3521",
      "metadata": {
        "id": "fecaf883-f724-4ba6-9a4b-7924da5e3521"
      },
      "source": [
        "![DT_1.png](attachment:27a6bacc-0a28-4c0f-8633-0a079722ee42.png)\n",
        "\n",
        "In the decision tree chart, each internal node has a decision rule that splits the data. Gini, referred to as Gini ratio, measures the impurity of the node. You can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node.\n",
        "\n",
        "Here, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let's optimize it by pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f91c191-d915-4c8c-b2ca-3601a0c0162a",
      "metadata": {
        "id": "2f91c191-d915-4c8c-b2ca-3601a0c0162a"
      },
      "source": [
        "## Optimizing Decision Tree Performance\n",
        "\n",
        " - criterion : optional (default=”gini”) or Choose attribute selection measure. This parameter allows us to use the different-different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
        "\n",
        " - splitter : string, optional (default=”best”) or Split Strategy. This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "\n",
        " - max_depth : int or None, optional (default=None) or Maximum Depth of a Tree. The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting (Source).\n",
        "\n",
        "In Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. Maximum depth of the tree can be used as a control variable for pre-pruning. In the following the example, you can plot a decision tree on the same data with max_depth=3. Other than pre-pruning parameters, You can also try other attribute selection measure such as entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9002b62d-8c8f-433a-aa9c-038588941b1b",
      "metadata": {
        "id": "9002b62d-8c8f-433a-aa9c-038588941b1b"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef7fffb-038c-4dee-8557-92d1f889c7c5",
      "metadata": {
        "id": "3ef7fffb-038c-4dee-8557-92d1f889c7c5"
      },
      "outputs": [],
      "source": [
        ">>\n",
        "Accuracy: 0.7705627705627706"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21bc7b9f-b0b9-42db-983c-a7525c2e6b38",
      "metadata": {
        "id": "21bc7b9f-b0b9-42db-983c-a7525c2e6b38"
      },
      "source": [
        "Well, the classification rate increased to 77.05%, which is better accuracy than the previous model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0933571d-8955-45de-9d74-c20b4ce44721",
      "metadata": {
        "id": "0933571d-8955-45de-9d74-c20b4ce44721"
      },
      "source": [
        "## Visualizing Decision Trees\n",
        "\n",
        "Let's make our decision tree a little easier to understand\n",
        "\n",
        "Here, we've completed the following steps:\n",
        "\n",
        " - Imported the required libraries.\n",
        " - Created a StringIO object called dot_data to hold the text representation of the decision tree.\n",
        " - Exported the decision tree to the dot format using the export_graphviz function and write the output to the dot_data buffer.\n",
        " - Created a pydotplus graph object from the dot format representation of the decision tree stored in the dot_data buffer.\n",
        " - Written the generated graph to a PNG file named \"diabetes.png\".\n",
        " - Displayed the generated PNG image of the decision tree using the Image object from the IPython.display module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783a9171-28a7-44be-86f0-609cf688ae02",
      "metadata": {
        "id": "783a9171-28a7-44be-86f0-609cf688ae02"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and functions\n",
        "from six import StringIO  # To handle string input/output\n",
        "from IPython.display import Image  # To display images in IPython notebooks\n",
        "from sklearn.tree import export_graphviz  # Function to export a decision tree in DOT format\n",
        "import pydotplus  # To handle DOT files and create graph images\n",
        "\n",
        "# Create a StringIO object to hold the DOT data\n",
        "dot_data = StringIO()\n",
        "\n",
        "# Export the decision tree (clf) to DOT format and write it to the StringIO object\n",
        "export_graphviz(clf, out_file=dot_data,\n",
        "                filled=True, rounded=True,  # Fill nodes with colors and round their corners\n",
        "                special_characters=True,  # Use special characters in the labels\n",
        "                feature_names=feature_cols,  # Use feature column names in the labels\n",
        "                class_names=['0', '1'])  # Class names for the target variable\n",
        "\n",
        "# Use pydotplus to create a graph from the DOT data\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "\n",
        "# Write the graph to a PNG file\n",
        "graph.write_png('diabetes.png')\n",
        "\n",
        "# Display the graph as a PNG image in the notebook\n",
        "Image(graph.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad7f9c75-7667-4966-8f94-897f8faf5c3a",
      "metadata": {
        "id": "ad7f9c75-7667-4966-8f94-897f8faf5c3a"
      },
      "source": [
        "![DT_2.png](attachment:043846db-0ff7-4197-8228-6e15bbecc383.png)\n",
        "\n",
        "As you can see, this pruned model is less complex, more explainable, and easier to understand than the previous decision tree model plot."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b138f4cd-1112-4715-a2f5-e9bc9fb041ad",
      "metadata": {
        "id": "b138f4cd-1112-4715-a2f5-e9bc9fb041ad"
      },
      "source": [
        "## Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008c521f-e842-4e93-be6a-872726a47603",
      "metadata": {
        "id": "008c521f-e842-4e93-be6a-872726a47603"
      },
      "source": [
        "### Exercice 1 : Feature Selection and Data Splitting\n",
        " - Select different features for building the decision tree model (e.g., exclude 'insulin' and 'skin' and include 'pregnant', 'glucose', 'bp', 'bmi', 'pedigree', and 'age').\n",
        " - Split the dataset into training and testing sets with a different test size of 20%."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data['Outcome']\n",
        "y = data['Outcome']  # Remplacez 'Outcome' par le nom de votre colonne cible si nécessaire\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Afficher les dimensions des ensembles\n",
        "print(\"Train set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsqQBiqV4mOu",
        "outputId": "995dfb06-9f1b-41d9-cda3-86470f4e9670"
      },
      "id": "rsqQBiqV4mOu",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set shape: (614,)\n",
            "Test set shape: (154,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59899e3b-af21-405d-8d20-9ab51897b580",
      "metadata": {
        "id": "59899e3b-af21-405d-8d20-9ab51897b580"
      },
      "source": [
        "### Exercice 2 : Building and Visualizing the Decision Tree\n",
        " - Build a decision tree using the 'entropy' criterion and set the maximum depth to 4.\n",
        " - Visualize the decision tree using the export_graphviz function and display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data['Outcome']\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construire l'arbre de décision\n",
        "clf = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualiser l'arbre de décision\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=features,\n",
        "                           class_names=['No Diabetes', 'Diabetes'],\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Enregistre l'arbre sous forme de fichier\n",
        "graph.view()  # Affiche l'arbre\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1N2I9WaD9g5r",
        "outputId": "c5434e54-4268-4709-a0c9-6f59b08d76bc"
      },
      "id": "1N2I9WaD9g5r",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-56e552e2d80b>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Construire l'arbre de décision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Visualiser l'arbre de décision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \"\"\"\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    250\u001b[0m             )\n\u001b[1;32m    251\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_X_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mcheck_X_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"estimator\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1048\u001b[0m                         \u001b[0;34m\"if it contains a single sample.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m                     )\n\u001b[0;32m-> 1050\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"USV\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected a 2-dimensional container but got <class 'pandas.core.series.Series'> instead. Pass a DataFrame containing a single row (i.e. single sample) or a single column (i.e. single feature) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319d6de9-f057-4f63-ae85-b3c4fd0c0dd6",
      "metadata": {
        "id": "319d6de9-f057-4f63-ae85-b3c4fd0c0dd6"
      },
      "source": [
        "### Exercice 3 : Optimizing and Evaluating the Model\n",
        " - Create a decision tree classifier with the 'gini' criterion and a maximum depth of 5.\n",
        " - Evaluate the model accuracy and compare it with the previous models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construire l'arbre de décision avec le critère 'gini'\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les résultats sur l'ensemble de test\n",
        "y_pred_gini = clf_gini.predict(X_test)\n",
        "\n",
        "# Évaluer la précision du modèle\n",
        "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "\n",
        "# Afficher la précision\n",
        "print(\"Accuracy of the Gini model:\", accuracy_gini)\n",
        "\n",
        "# Comparer avec le modèle précédent (entropy)\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=42)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "\n",
        "print(\"Accuracy of the Entropy model:\", accuracy_entropy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ndULElCsknDn",
        "outputId": "00dc51b7-3b6f-43b0-b68b-9af6c30abe89"
      },
      "id": "ndULElCsknDn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'diabetes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3a0bffe72cf2>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Charger le dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'diabetes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Sélectionner les caractéristiques souhaitées\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317f8c0a-e674-4c56-b7cb-e556dff4a145",
      "metadata": {
        "id": "317f8c0a-e674-4c56-b7cb-e556dff4a145"
      },
      "source": [
        "### Exercice 4 : Adjusting the Splitter Parameter\n",
        " - Create a decision tree classifier with the 'best' splitter and a maximum depth of 3.\n",
        " - Create another decision tree classifier with the 'random' splitter and a maximum depth of 3. Compare the accuracy of both models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classificateur avec le séparateur 'best'\n",
        "clf_best = DecisionTreeClassifier(splitter='best', max_depth=3, random_state=42)\n",
        "clf_best.fit(X_train, y_train)\n",
        "y_pred_best = clf_best.predict(X_test)\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "# Classificateur avec le séparateur 'random'\n",
        "clf_random = DecisionTreeClassifier(splitter='random', max_depth=3, random_state=42)\n",
        "clf_random.fit(X_train, y_train)\n",
        "y_pred_random = clf_random.predict(X_test)\n",
        "accuracy_random = accuracy_score(y_test, y_pred_random)\n",
        "\n",
        "# Afficher les précisions\n",
        "print(\"Accuracy of the Best Splitter model:\", accuracy_best)\n",
        "print(\"Accuracy of the Random Splitter model:\", accuracy_random)\n"
      ],
      "metadata": {
        "id": "1YrvDPXflw_p"
      },
      "id": "1YrvDPXflw_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8ff65f6-5d7e-484b-90a2-cb4b296a3879",
      "metadata": {
        "id": "d8ff65f6-5d7e-484b-90a2-cb4b296a3879"
      },
      "source": [
        "### Exercice 5 : Exploring the Effect of max_features Parameter\n",
        " - Create a decision tree classifier with max_features set to 'auto' and max depth of 4.\n",
        " - Create another decision tree classifier with max_features set to 3 and max depth of 4. Compare the accuracy of both models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classificateur avec max_features='auto'\n",
        "clf_auto = DecisionTreeClassifier(max_features='auto', max_depth=4, random_state=42)\n",
        "clf_auto.fit(X_train, y_train)\n",
        "y_pred_auto = clf_auto.predict(X_test)\n",
        "accuracy_auto = accuracy_score(y_test, y_pred_auto)\n",
        "\n",
        "# Classificateur avec max_features=3\n",
        "clf_3 = DecisionTreeClassifier(max_features=3, max_depth=4, random_state=42)\n",
        "clf_3.fit(X_train, y_train)\n",
        "y_pred_3 = clf_3.predict(X_test)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
        "\n",
        "# Afficher les précisions\n",
        "print(\"Accuracy of the model with max_features='auto':\", accuracy_auto)\n",
        "print(\"Accuracy of the model with max_features=3:\", accuracy_3)\n"
      ],
      "metadata": {
        "id": "YLXfNV2xl5GI"
      },
      "id": "YLXfNV2xl5GI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e34ba6cb-d34c-48c6-8c54-09e802c82ad0",
      "metadata": {
        "id": "e34ba6cb-d34c-48c6-8c54-09e802c82ad0"
      },
      "source": [
        "### Exercice 6 : Evaluating Model Performance with Cross-Validation\n",
        " - Use cross-validation to evaluate the performance of the decision tree model with 'gini' criterion and max depth of 4.\n",
        " - Compare the cross-validation accuracy with the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classificateur avec le critère 'gini' et profondeur maximale de 4\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=42)\n",
        "\n",
        "# Évaluer la précision avec la validation croisée\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
        "mean_cv_accuracy = cv_scores.mean()\n",
        "\n",
        "# Entraîner le modèle sur l'ensemble d'entraînement\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les résultats sur l'ensemble de test\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Évaluer la précision sur l'ensemble de test\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(\"Mean Cross-Validation Accuracy:\", mean_cv_accuracy)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "_ojoL-dEmGrN"
      },
      "id": "_ojoL-dEmGrN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "07f57f7e-a5dc-4cf6-8c78-7db0ae590baf",
      "metadata": {
        "id": "07f57f7e-a5dc-4cf6-8c78-7db0ae590baf"
      },
      "source": [
        "### Exercice 7 : Using Class Weights to Handle Imbalanced Data\n",
        " - Create a decision tree classifier with class_weight set to 'balanced' and max depth of 4.\n",
        " - Evaluate the model accuracy and compare it with the model without class weighting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classificateur sans pondération des classes\n",
        "clf_no_weight = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "clf_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = clf_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Classificateur avec pondération des classes\n",
        "clf_balanced = DecisionTreeClassifier(class_weight='balanced', max_depth=4, random_state=42)\n",
        "clf_balanced.fit(X_train, y_train)\n",
        "y_pred_balanced = clf_balanced.predict(X_test)\n",
        "accuracy_balanced = accuracy_score(y_test, y_pred_balanced)\n",
        "\n",
        "# Afficher les précisions\n",
        "print(\"Accuracy of the model without class weighting:\", accuracy_no_weight)\n",
        "print(\"Accuracy of the model with class weighting (balanced):\", accuracy_balanced)\n"
      ],
      "metadata": {
        "id": "9e5EDe45mRHZ"
      },
      "id": "9e5EDe45mRHZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3f5aa4c6-bfd5-4865-bdab-a9e9b9000fd2",
      "metadata": {
        "id": "3f5aa4c6-bfd5-4865-bdab-a9e9b9000fd2"
      },
      "source": [
        "### Exercice 8 : Using Different Criteria\n",
        " - Create a decision tree classifier with the 'entropy' criterion and no limit on the depth.\n",
        " - Evaluate the model accuracy and compare it with a model using the 'gini' criterion with no limit on the depth."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Classificateur avec le critère 'entropy' et sans limite de profondeur\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "\n",
        "# Classificateur avec le critère 'gini' et sans limite de profondeur\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_gini.fit(X_train, y_train)\n",
        "y_pred_gini = clf_gini.predict(X_test)\n",
        "accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "\n",
        "# Afficher les précisions\n",
        "print(\"Accuracy of the model with 'entropy' criterion:\", accuracy_entropy)\n",
        "print(\"Accuracy of the model with 'gini' criterion:\", accuracy_gini)\n"
      ],
      "metadata": {
        "id": "_G8Pn55XmaON"
      },
      "id": "_G8Pn55XmaON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f834a115-8a27-47e1-8a8e-7e5a205df584",
      "metadata": {
        "id": "f834a115-8a27-47e1-8a8e-7e5a205df584"
      },
      "source": [
        "### Exercice 9 : Feature Importance\n",
        " - Train a decision tree classifier with the default parameters.\n",
        " - Extract and display the feature importances. Which features are the most important for the classification?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraîner le classificateur avec les paramètres par défaut\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Extraire les importances des caractéristiques\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Créer un DataFrame pour les importances\n",
        "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "\n",
        "# Trier les importances\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Afficher les importances des caractéristiques\n",
        "print(importance_df)\n",
        "\n",
        "# Visualiser les importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uliRQH0OmlW2"
      },
      "id": "uliRQH0OmlW2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b1569127-dc2a-4ac5-828e-9630f340d9f1",
      "metadata": {
        "id": "b1569127-dc2a-4ac5-828e-9630f340d9f1"
      },
      "source": [
        "### Exercice 10 : Visualizing Different Depth Trees\n",
        "\n",
        "*   Élément de liste\n",
        "*   Élément de liste\n",
        "\n",
        "\n",
        " - Create and visualize a decision tree with a maximum depth of 2.\n",
        " - Create and visualize a decision tree with a maximum depth of 5. Compare the complexity of the trees."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Sélectionner les caractéristiques souhaitées\n",
        "features = ['pregnant', 'glucose', 'bp', 'bmi', 'pedigree', 'age']\n",
        "X = data[features]\n",
        "y = data['Outcome']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Créer et entraîner l'arbre de décision avec une profondeur maximale de 2\n",
        "clf_depth_2 = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
        "clf_depth_2.fit(X_train, y_train)\n",
        "\n",
        "# Visualiser l'arbre de décision avec une profondeur maximale de 2\n",
        "dot_data_depth_2 = export_graphviz(clf_depth_2, out_file=None,\n",
        "                                    feature_names=features,\n",
        "                                    class_names=['No Diabetes', 'Diabetes'],\n",
        "                                    filled=True, rounded=True,\n",
        "                                    special_characters=True)\n",
        "graph_depth_2 = graphviz.Source(dot_data_depth_2)\n",
        "graph_depth_2.render(\"decision_tree_depth_2\")  # Enregistre l'arbre sous forme de fichier\n",
        "graph_depth_2.view()  # Affiche l'arbre\n",
        "\n",
        "# Créer et entraîner l'arbre de décision avec une profondeur maximale de 5\n",
        "clf_depth_5 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "clf_depth_5.fit(X_train, y_train)\n",
        "\n",
        "# Visualiser l'arbre de décision avec une profondeur maximale de 5\n",
        "dot_data_depth_5 = export_graphviz(clf_depth_5, out_file=None,\n",
        "                                    feature_names=features,\n",
        "                                    class_names=['No Diabetes', 'Diabetes'],\n",
        "                                    filled=True, rounded=True,\n",
        "                                    special_characters=True)\n",
        "graph_depth_5 = graphviz.Source(dot_data_depth_5)\n",
        "graph_depth_5.render(\"decision_tree_depth_5\")  # Enregistre l'arbre sous forme de fichier\n",
        "graph_depth_5.view()  # Affiche l'arbre\n"
      ],
      "metadata": {
        "id": "thbsrzNtm5CK"
      },
      "id": "thbsrzNtm5CK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d3868795-c87a-439f-a550-36377f032eff",
      "metadata": {
        "id": "d3868795-c87a-439f-a550-36377f032eff"
      },
      "source": [
        "### Exercice 11 : Handling Missing Values\n",
        " - Impute missing values in the dataset using the mean of each feature.\n",
        " - Train and evaluate a decision tree classifier on the dataset with imputed values. Compare the accuracy with the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Vérifier les valeurs manquantes\n",
        "print(\"Missing values in original dataset:\\n\", data.isnull().sum())\n",
        "\n",
        "# Imputer les valeurs manquantes avec la moyenne\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# Diviser le dataset original en ensembles d'entraînement et de test\n",
        "X_original = data.drop('Outcome', axis=1)\n",
        "y_original = data['Outcome']\n",
        "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original, y_original, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraîner le modèle sur le dataset original\n",
        "clf_orig = DecisionTreeClassifier(random_state=42)\n",
        "clf_orig.fit(X_train_orig, y_train_orig)\n",
        "y_pred_orig = clf_orig.predict(X_test_orig)\n",
        "accuracy_orig = accuracy_score(y_test_orig, y_pred_orig)\n",
        "\n",
        "# Diviser le dataset imputé en ensembles d'entraînement et de test\n",
        "X_imputed = data_imputed.drop('Outcome', axis=1)\n",
        "y_imputed = data_imputed['Outcome']\n",
        "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_imputed, y_imputed, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entraîner le modèle sur le dataset imputé\n",
        "clf_imp = DecisionTreeClassifier(random_state=42)\n",
        "clf_imp.fit(X_train_imp, y_train_imp)\n",
        "y_pred_imp = clf_imp.predict(X_test_imp)\n",
        "accuracy_imp = accuracy_score(y_test_imp, y_pred_imp)\n",
        "\n",
        "# Afficher les précisions\n",
        "print(\"Accuracy of the model on the original dataset:\", accuracy_orig)\n",
        "print(\"Accuracy of the model on the imputed dataset:\", accuracy_imp)\n"
      ],
      "metadata": {
        "id": "xr5Bgml6m_lm"
      },
      "id": "xr5Bgml6m_lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6760ee8f-2d10-498c-a6d7-7d12f32cf23e",
      "metadata": {
        "id": "6760ee8f-2d10-498c-a6d7-7d12f32cf23e"
      },
      "source": [
        "### Exercice 12 : Decision Tree Classification on the \"Iris\" Dataset\n",
        "Perform Decision Tree classification on the Iris dataset to classify the type of iris flower. Experiment with different tree parameters to optimize the model's performance.\n",
        "\n",
        "Instructions:\n",
        "- Load the Iris Dataset: Use the `load_iris()` function from `sklearn.datasets` to load the Iris dataset.\n",
        "- Experiment with Different Criteria: Modify the code to use 'entropy' and 'gini' criteria. Observe the changes in accuracy.\n",
        "- Tune the Max Depth Parameter: Experiment with different values of the max depth parameter (e.g., max_depth=2, max_depth=4, max_depth=6). Observe how it affects the model's performance.\n",
        "- Visualize the Decision Tree: Visualize the decision tree using the `export_graphviz` function and display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fonction pour entraîner et évaluer le modèle\n",
        "def evaluate_model(criterion, max_depth):\n",
        "    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy, clf\n",
        "\n",
        "# Expérimenter avec différents critères et profondeurs\n",
        "results = []\n",
        "for criterion in ['gini', 'entropy']:\n",
        "    for max_depth in [2, 4, 6]:\n",
        "        accuracy, model = evaluate_model(criterion, max_depth)\n",
        "        results.append((criterion, max_depth, accuracy))\n",
        "\n",
        "        # Visualiser l'arbre de décision\n",
        "        dot_data = export_graphviz(model, out_file=None,\n",
        "                                   feature_names=feature_names,\n",
        "                                   class_names=class_names,\n",
        "                                   filled=True, rounded=True,\n",
        "                                   special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(f\"iris_decision_tree_{criterion}_{max_depth}\")  # Enregistre l'arbre sous forme de fichier\n",
        "        graph.view()  # Affiche l'arbre\n",
        "\n",
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results, columns=['Criterion', 'Max Depth', 'Accuracy'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "xZ9aM44Kneru"
      },
      "id": "xZ9aM44Kneru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "957d1aad-d0b2-4ded-99fe-06f818def475",
      "metadata": {
        "id": "957d1aad-d0b2-4ded-99fe-06f818def475"
      },
      "source": [
        "### Exercice 13 : Decision Tree Classification on the \"Breast Cancer\" Dataset\n",
        "Perform Decision Tree classification on the Breast Cancer dataset to classify whether a tumor is malignant or benign. Experiment with different tree parameters to optimize the model's performance.\n",
        "\n",
        "Instructions:\n",
        "- Load the Breast Cancer Dataset: Use the `load_breast_cancer()` function from `sklearn.datasets` to load the Breast Cancer dataset.\n",
        "- Experiment with Different Criteria: Modify the code to use 'entropy' and 'gini' criteria. Observe the changes in accuracy.\n",
        "- Tune the Min Samples Split Parameter: Experiment with different values of the min_samples_split parameter (e.g., min_samples_split=2, min_samples_split=10, min_samples_split=20). Observe how it affects the model's performance.\n",
        "- Visualize the Decision Tree: Visualize the decision tree using the `export_graphviz` function and display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset du cancer du sein\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "feature_names = cancer.feature_names\n",
        "class_names = cancer.target_names\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fonction pour entraîner et évaluer le modèle\n",
        "def evaluate_model(criterion, min_samples_split):\n",
        "    clf = DecisionTreeClassifier(criterion=criterion, min_samples_split=min_samples_split, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy, clf\n",
        "\n",
        "# Expérimenter avec différents critères et min_samples_split\n",
        "results = []\n",
        "for criterion in ['gini', 'entropy']:\n",
        "    for min_samples_split in [2, 10, 20]:\n",
        "        accuracy, model = evaluate_model(criterion, min_samples_split)\n",
        "        results.append((criterion, min_samples_split, accuracy))\n",
        "\n",
        "        # Visualiser l'arbre de décision\n",
        "        dot_data = export_graphviz(model, out_file=None,\n",
        "                                   feature_names=feature_names,\n",
        "                                   class_names=class_names,\n",
        "                                   filled=True, rounded=True,\n",
        "                                   special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(f\"breast_cancer_decision_tree_{criterion}_{min_samples_split}\")  # Enregistre l'arbre sous forme de fichier\n",
        "        graph.view()  # Affiche l'arbre\n",
        "\n",
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results, columns=['Criterion', 'Min Samples Split', 'Accuracy'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "uM0_3Um3nnIx"
      },
      "id": "uM0_3Um3nnIx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c0dc81d-c922-45a1-b4b7-de9f4eddc37b",
      "metadata": {
        "id": "9c0dc81d-c922-45a1-b4b7-de9f4eddc37b"
      },
      "source": [
        "### Exercice 14 : Decision Tree Classification on the \"Digits\" Dataset\n",
        "Perform Decision Tree classification on the Digits dataset to classify the handwritten digits. Experiment with different tree parameters to optimize the model's performance.\n",
        "\n",
        "Instructions:\n",
        "- Load the Digits Dataset: Use the `load_digits()` function from `sklearn.datasets` to load the Digits dataset.\n",
        "- Experiment with Different Criteria: Modify the code to use 'entropy' and 'gini' criteria. Observe the changes in accuracy.\n",
        "- Tune the Max Leaf Nodes Parameter: Experiment with different values of the max_leaf_nodes parameter (e.g., max_leaf_nodes=10, max_leaf_nodes=20, max_leaf_nodes=30). Observe how it affects the model's performance.\n",
        "- Visualize the Decision Tree: Visualize the decision tree using the `export_graphviz` function and display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Charger le dataset des chiffres manuscrits\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "feature_names = digits.feature_names\n",
        "class_names = [str(i) for i in range(10)]\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fonction pour entraîner et évaluer le modèle\n",
        "def evaluate_model(criterion, max_leaf_nodes):\n",
        "    clf = DecisionTreeClassifier(criterion=criterion, max_leaf_nodes=max_leaf_nodes, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy, clf\n",
        "\n",
        "# Expérimenter avec différents critères et max_leaf_nodes\n",
        "results = []\n",
        "for criterion in ['gini', 'entropy']:\n",
        "    for max_leaf_nodes in [10, 20, 30]:\n",
        "        accuracy, model = evaluate_model(criterion, max_leaf_nodes)\n",
        "        results.append((criterion, max_leaf_nodes, accuracy))\n",
        "\n",
        "        # Visualiser l'arbre de décision\n",
        "        dot_data = export_graphviz(model, out_file=None,\n",
        "                                   feature_names=feature_names,\n",
        "                                   class_names=class_names,\n",
        "                                   filled=True, rounded=True,\n",
        "                                   special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(f\"digits_decision_tree_{criterion}_{max_leaf_nodes}\")  # Enregistre l'arbre sous forme de fichier\n",
        "        graph.view()  # Affiche l'arbre\n",
        "\n",
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results, columns=['Criterion', 'Max Leaf Nodes', 'Accuracy'])\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "phNcDUWZoNPO"
      },
      "id": "phNcDUWZoNPO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f4de96f1-7ff6-4930-ae75-70dcd8f23c44",
      "metadata": {
        "id": "f4de96f1-7ff6-4930-ae75-70dcd8f23c44"
      },
      "source": [
        "### Exercice 15 : Decision Tree Classification on the \"Titanic\" Dataset\n",
        "Perform Decision Tree classification on the Titanic dataset to predict passenger survival. Experiment with different tree parameters to optimize the model's performance.\n",
        "\n",
        "Instructions:\n",
        "- Load the Titanic Dataset: Use the `sns.load_dataset('titanic')` function from the seaborn library to load the Titanic dataset.\n",
        "- Preprocess the Data: Handle missing values and encode categorical variables.\n",
        "- Experiment with Different Criteria: Modify the code to use 'entropy' and 'gini' criteria. Observe the changes in accuracy.\n",
        "- Tune the Max Depth Parameter: Experiment with different values of the max depth parameter (e.g., max_depth=2, max_depth=4, max_depth=6). Observe how it affects the model's performance.\n",
        "- Visualize the Decision Tree: Visualize the decision tree using the `export_graphviz` function and display the resulting image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Charger le dataset Titanic\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Prétraitement des données\n",
        "# Gérer les valeurs manquantes\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "# Fix: Use titanic['embarked'] instead of titanic[['embarked']] to pass a Series\n",
        "titanic['embarked'] = imputer.fit_transform(titanic['embarked'].values.reshape(-1, 1))[:,0]\n",
        "titanic['age'] = imputer.fit_transform(titanic['age'].values.reshape(-1, 1))[:,0]\n",
        "\n",
        "# Encoder les variables catégorielles\n",
        "label_encoder = LabelEncoder()\n",
        "titanic['sex'] = label_encoder.fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = label_encoder.fit_transform(titanic['embarked'])\n",
        "\n",
        "# Sélectionner les caractéristiques et la cible\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']\n",
        "X = titanic[features]\n",
        "y = titanic['survived']\n",
        "\n",
        "# Diviser le dataset en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fonction pour entraîner et évaluer le modèle\n",
        "def evaluate_model(criterion, max_depth):\n",
        "    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy, clf\n",
        "\n",
        "# Expérimenter avec différents critères et profondeurs maximales\n",
        "results = []\n",
        "for criterion in ['gini', 'entropy']:\n",
        "    for max_depth in [2, 4, 6]:\n",
        "        accuracy, model = evaluate_model(criterion, max_depth)\n",
        "        results.append((criterion, max_depth, accuracy))\n",
        "\n",
        "        # Visualiser l'arbre de décision\n",
        "        dot_data = export_graphviz(model, out_file=None,\n",
        "                                   feature_names=features,\n",
        "                                   class_names=['Not Survived', 'Survived'],\n",
        "                                   filled=True, rounded=True,\n",
        "                                   special_characters=True)\n",
        "        graph = graphviz.Source(dot_data)\n",
        "        graph.render(f\"titanic_decision_tree_{criterion}_{max_depth}\")  # Enregistre l'arbre sous forme de fichier\n",
        "        graph.view()  # Affiche l'arbre\n",
        "\n",
        "# Afficher les résultats\n",
        "results_df = pd.DataFrame(results, columns=['Criterion', 'Max Depth', 'Accuracy'])\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWHkyYy2oVAi",
        "outputId": "f18ecf0e-f600-4e75-a0af-6e231a8747c0"
      },
      "id": "QWHkyYy2oVAi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Criterion  Max Depth  Accuracy\n",
            "0      gini          2  0.765363\n",
            "1      gini          4  0.798883\n",
            "2      gini          6  0.810056\n",
            "3   entropy          2  0.765363\n",
            "4   entropy          4  0.798883\n",
            "5   entropy          6  0.770950\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}